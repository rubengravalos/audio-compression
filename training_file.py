# -*- coding: utf-8 -*-
"""COD_SpeechFile.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X9TWM1_HmD00We4l2kiVlVaX7b0an-sF
"""

import time
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import numpy as np
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as data
import math
import pickle
from IPython.display import Audio

"""
Parameter definition
"""

#N = 5000
#T = 100

input_dim = 1
output_dim = 256
hidden_dim = 256
nb_lstm_layers = 2
nb_epochs = 10
batch_size = 32

print('  input dimension: %d' % input_dim)
print('  hidden dimension: %d' % hidden_dim)
print('  output dimension: %d' % output_dim)
print('  number of LSTM layers: %d' % nb_lstm_layers)
print('  number of epochs: %d' % nb_epochs)
print('  batch size: %d' % batch_size)

"""
Data -> Artificial data
This is sequence regression example, input is continous and output is continous.

!wget http://dihana.cps.unizar.es/~cadrete/speech.pkl --no-check-certificate
"""

with open('/home/alumnos/alumno3/work/TFM/experimento2capas/speech.pkl','rb') as f:
    x = pickle.load(f)

#print(x.shape)

T = len(x[0])
N = len(x)
#print("N muestras/señal: ", T)
#print("N señales: ", N)

def dec(x, M=256):
    mu = M - 1
    y = x.astype(np.float32)
    y = 2 * (y / mu) - 1
    x = np.sign(y) * (1.0 / mu) * ((1.0 + mu)**abs(y) - 1.0)
    return x

x[:,0] = 0

#print(x.min(), x.max())

fs = 8000
x_manipulate = dec(x)


"""
plt.figure()
plt.plot(x_manipulate[0,:])
plt.savefig('audiosignal.png')
Audio(x_manipulate[0,:],rate=fs)
"""

"""
Train
Train, Dev split
"""

x = x.reshape(N, T, input_dim)
x_test = x[:100]
x_dev = x[100:200]
x_train = x[200:]
#x_dev_aux = x_dev[:10,:100]
#x_train_aux = x_train[:10,:100]


print('  x_test: %s (%s)' % (x_test.shape, x_test.dtype))
print('  x_dev: %s (%s)' % (x_dev.shape, x_dev.dtype))
print('  x_train: %s (%s)' % (x_train.shape, x_train.dtype))
#print('  x_dev_aux: %s (%s)' % (x_dev_aux.shape, x_dev_aux.dtype))
#print('  x_train_aux: %s (%s)' % (x_train_aux.shape, x_train_aux.dtype))


"""
Net
"""
class Net(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, nb_lstm_layers, dropout=0.5):
        super(Net, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, nb_lstm_layers, batch_first=True, bidirectional=False, dropout=dropout)
        self.fc = nn.Linear( hidden_dim, output_dim)
        self.J = nn.CrossEntropyLoss()

    def forward(self, x):
        x = x.float()
        x, state = self.lstm(x)
        x = self.fc(x)
        return x

    def loss(self, out, y): 
        n, t, d = y.shape
        return self.J(out.reshape(n*t,-1), y.reshape(n*t,))

    def predict(self, x, state=None):
        x = x.float()
        x, state = self.lstm(x, state)
        x = self.fc(x)
        return x.softmax(-1), state


model = Net(input_dim, hidden_dim, output_dim, nb_lstm_layers)
model.cuda()
nb_param = sum(p.numel() for p in model.parameters())
print(model)
print('# param:    %d' % nb_param)


"""
Optimizer
Scheduler decreases lr every epoch by a factor of 0.1
"""

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=5e-4)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)


"""
## Train
"""


trainloader = data.DataLoader(x_train, batch_size=batch_size, shuffle=True, num_workers=0)
devloader = data.DataLoader(x_dev, batch_size=1, shuffle=False, num_workers=0)

loss_train = np.zeros(nb_epochs)
loss_dev = np.zeros(nb_epochs)

model.train()
for i in range(nb_epochs):
    if i > 0:
        scheduler.step()

    # ---- W
    tic = time.time()
    model.train()
    i=1
    for x in trainloader:
        x = x.cuda()
        y = x[:,1:]      # value to predict
        x = x[:,:-1]     # previous values

        """
        if i == 1:
            print(type(x))
            try_train = x[:1].reshape(8000, -1)
            plt.plot(try_train)
            plt.title("Training signal shape")
            #plt.show()
            plt.savefig('training_shape.png')
            i=0
        """

        optimizer.zero_grad()
        out = model(x)
        #print(out.shape)
        loss = model.loss(out, y.long())
        loss.backward()
        optimizer.step()

        loss_train[i] += loss.item() /  (len(x_dev) * x.shape[0] * x.shape[1])
    toc = time.time()

    # ---- print
    #print("it %d/%d, Jtr = %f, time: %.2fs" % (i, nb_epochs, loss_train[i], toc - tic))

    # ---- dev
    model.eval()
    for x in devloader:
        x = x.cuda()

        y = x[:,1:]      # value to predict
        x = x[:,:-1]     # previous values

        out = model(x)
        loss = model.loss(out, y.long())

        loss_dev[i] += loss.item() / (len(x_dev) * x.shape[0] * x.shape[1])

        #print('    Jdev = %f, err = %f\n' % (loss_dev[i], err_dev[i]))
    #print('    Jdev = %f\n' % loss_dev[i])


## View loss vs its

plt.figure()
plt.plot(loss_train, 'r')
plt.plot(loss_dev, 'bo--')
plt.ylabel('J')
plt.xlabel('it')
plt.grid(True)
#plt.show()
plt.savefig('training_losses_2L_10B_64MB.png')

## Save model

torch.save( model, 'model.dat')

"""
Test the model by running "testing_file.py"
"""
"""
Clase que proporciona los métodos necesarios para codificar un mensaje 
utilizando el método Huffman.
"""

class HuffTree(object):
    def __init__(self, w, symbol=None, zero=None, one=None):
        self.w = w
        self.symbol = symbol #symbol only ever populated on leafs
        self.zero = zero
        self.one = one
    
def _combine(tree1, tree2):
    return HuffTree(w = tree1.w+tree2.w, zero=tree1, one=tree2)

def make_huffman_tree(w_):
    return _build([HuffTree(w, i) for i, w in w_] )
    #return _build([HuffTree(w, i) for i, w in enumerate(w_)] )

def _build(nodes):
    if len(nodes) == 1:
        #bottom out if only one node left
        return nodes[0]
    else:
        #otherwise sort the list, take the bottom two elements and combine
        nodes.sort(key = lambda x: x.w, reverse=True)
        n1 = nodes.pop()
        n2 = nodes.pop()
        nodes.append(_combine(n2, n1))
        return _build(nodes)


def flatten_to_dict(tree, codeword=[], code_dict={}):
    if not tree.symbol is None:
        if len(codeword) == 0:
            codeword.append(0)
        code_dict[tree.symbol] = codeword
    else:
        flatten_to_dict(tree.zero, codeword + [0,], code_dict)
        flatten_to_dict(tree.one, codeword + [1,], code_dict)
    return code_dict

def huffman_encode(m_, d):
    return sum([d[m] for m in m_],[])

def huffman_decode(code_message, hufftree):
    decoded=[]
    t = hufftree
    for i in code_message:
        if i == 0:
            t = t.zero
        elif i == 1:
            t = t.one
        else:
            raise Exception("Code_message not binary")

        if not t.symbol is None:
            decoded.append(t.symbol)
            t = hufftree

    return decoded

from math import log2
"""
Clase que permite codificar y decodificar el mensaje, además de comprobar que 
el mensaje original con el decodificado coinciden.
"""

class CodDecod(object):
    def __init__(self, x_true, x_t0, symbols, model):
            if next(model.parameters()).is_cuda:
                self.x_true = x_true.cuda()
                self.x_t0 = x_t0.cuda()
            else:
                self.x_true = x_true
                self.x_t0 = x_t0
            self.symbols = symbols
            self.model = model

    def Code(self):
        s = None
        message_encoded = []
        x_t = self.x_t0
        print("Starting coding process")
        for t in range(len(self.x_true[0])):
            p, s = self.model.predict(x_t, s)
            if next(model.parameters()).is_cuda:
                x_t = (torch.FloatTensor([[[self.x_true[0,t]]]])).cuda()
            else:
                x_t = torch.FloatTensor([[[self.x_true[0,t]]]])
            w_ = p.reshape((len(p[0,0]), -1))
            w_ = w_.tolist()
            dictionary = [ (self.symbols[j], w_[j]) for j in range(len(w_)) ]
            h = make_huffman_tree(dictionary)
            
            d = flatten_to_dict(h)
            w = [int(x_true[0, t])]
            u = huffman_encode(w, d)
        
            message_encoded.append(u)
            if t==0 or t==20:
                print('----------------------------------')
                print('diccionario: ', d)
                cont = 0
                for a in d:
                    print(a,":",d[a])
                    cont += 1
                    if cont >= 5:
                        break

                print('w: ',w)
                print('palabra codificado: ',u)
                print("x_t: ", x_t)
                print('----------------------------------')
            print("Coding process finished")

        message_encoded = [item for sublist in message_encoded for item in sublist]
        return message_encoded

    def Decode(self, message_encoded):
        x_t = self.x_t0
        s = None
        message_decoded = []
        word_encoded = []
        i = 0
        print("Starting decoding process")
        for t in range(len(self.x_true[0])):
            p, s = self.model.predict(x_t, s)
            w_ = p.reshape((len(p[0,0]), -1))
            w_ = w_.tolist()
            dictionary = [ (self.symbols[j], w_[j]) for j in range(len(w_)) ]
            h = make_huffman_tree(dictionary) 
            d = flatten_to_dict(h)
            
            word_encoded = []
            word_encoded.extend(message_encoded[i:i+1])
            while ( word_encoded not in d.values() ) and ( i < len(message_encoded)-1 ):
                i = i+1
                word_encoded.extend(message_encoded[i:i+1])
            i = i+1
            word_decoded = huffman_decode(word_encoded, h)
            message_decoded.append(word_decoded)
            if next(model.parameters()).is_cuda:
                x_t = (torch.FloatTensor([[word_decoded]])).cuda()
            else:
                x_t = torch.FloatTensor([[word_decoded]])
            
        
        print("Decoding process finished")
        return message_decoded

    def Check(self, message_decoded):
        # ANÁLISIS DE RESULTADOS
        x_true = self.x_true[0].cpu().detach().numpy()
        message_decoded = np.array(message_decoded)
        err = abs(message_decoded-x_true)
        plt.plot(err)
        plt.title("Absolute error at each sample")
        #plt.show()
        plt.savefig('/2L_10B_16MB/absolute_sample_error.png')

        fig = plt.figure()
        ax = fig.add_subplot(1, 1, 1)
        ax.plot(message_decoded, color ='tab:red')
        ax.plot(x_true, color ='tab:blue')

        ax.set_title('Decoded voice representation')
        
        # display the plot
        #plt.show()
        plt.savefig('/2L_10B_16MB/decoded_voice_representation.png')

    def DataCompressionRate(self, message_encoded, output_dim):
        original_bits_sample = math.log2(output_dim)
        encoded_bits_sample = len(message_encoded)/len(self.x_true[0])
        compression_rate = original_bits_sample/encoded_bits_sample
        print("Longitud del mensaje sin codificar: " + str(len(self.x_true[0])))
        print("Longitud del mensaje codificado: " + str(len(message_encoded)))
        print("Original size: ", original_bits_sample, "bits/sample")
        print("Encoded size: ", encoded_bits_sample, "bits/sample")
        print("Compression rate: ", compression_rate)
        return compression_rate

x_true = torch.FloatTensor( x_test[:5, :100] )
print(len(x_true))
print(x_true.shape)
"""plt.plot(x_true[:1,:,:1])
plt.title("Test signal shape")
#plt.show()
plt.savefig('test_shape.png')"""
x_t0 = torch.FloatTensor([[[1]]])
symbols = [i for i in range (output_dim)]
rate = 0

#tic = time.time()
model.eval()
for i in range(len(x_true)):
    print("Signal " + str(i))
    CoderDecoder = CodDecod(x_true[i:i+1], x_t0, symbols, model)
    message_encoded = CoderDecoder.Code()
    message_decoded = CoderDecoder.Decode(message_encoded)
    #CoderDecoder.Check(message_decoded)

    rate += (CoderDecoder.DataCompressionRate(message_encoded, output_dim))/len(x_true)
#tac = time.time()
#tiempo_codificacion = tac-tic

print("Compression rate average: ", rate)
#print("Tiempo de codificación: ", tiempo_codificacion, "s")
