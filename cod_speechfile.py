# -*- coding: utf-8 -*-
"""COD_SpeechFile.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X9TWM1_HmD00We4l2kiVlVaX7b0an-sF
"""

import time
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as data
import math
import pickle
from IPython.display import Audio

"""
Parameter definition
"""

#N = 5000
#T = 100

input_dim = 1
output_dim = 256
hidden_dim = 256
nb_lstm_layers = 2
#nb_epochs = 100
nb_epochs = 10
batch_size = 32

print('  input dimension: %d' % input_dim)
print('  hidden dimension: %d' % hidden_dim)
print('  output dimension: %d' % output_dim)
print('  number of LSTM layers: %d' % nb_lstm_layers)
print('  number of epochs: %d' % nb_epochs)
print('  batch size: %d' % batch_size)

"""
Data -> Artificial data
This is sequence regression example, input is continous and output is continous.
"""
#!wget http://dihana.cps.unizar.es/~cadrete/speech.pkl --no-check-certificate

with open('speech.pkl','rb') as f:
    x = pickle.load(f)

print(x.shape)

T = len(x[0])
N = len(x)
print("N muestras/señal: ", T)
print("N señales: ", N)

def dec(x, M=256):
    mu = M - 1
    y = x.astype(np.float32)
    y = 2 * (y / mu) - 1
    x = np.sign(y) * (1.0 / mu) * ((1.0 + mu)**abs(y) - 1.0)
    return x

x[:,0] = 0

print(x.min(), x.max())

fs = 8000
x_manipulate = dec(x)

#plt.plot(x_manipulate[0,:])
Audio(x_manipulate[0,:],rate=fs)

"""
Train
Train, Dev split
"""

x = x.reshape(N, T, input_dim)
x_test = x[:100]
x_dev = x[100:200]
x_train = x[200:]
x_dev_aux = x_dev[:10,:100]
x_train_aux = x_train[:10,:100]


print('  x_test: %s (%s)' % (x_test.shape, x_test.dtype))
print('  x_dev: %s (%s)' % (x_dev.shape, x_dev.dtype))
print('  x_train: %s (%s)' % (x_train.shape, x_train.dtype))
print('  x_dev_aux: %s (%s)' % (x_dev_aux.shape, x_dev_aux.dtype))
print('  x_train_aux: %s (%s)' % (x_train_aux.shape, x_train_aux.dtype))

"""
Net
"""

class Net(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, nb_lstm_layers, dropout=0.5):
        super(Net, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, nb_lstm_layers, batch_first=True, bidirectional=False, dropout=dropout)
        self.fc = nn.Linear( hidden_dim, output_dim)
        self.J = nn.CrossEntropyLoss()

    def forward(self, x):
        x = x.float()
        x, state = self.lstm(x)
        x = self.fc(x)
        return x

    def loss(self, out, y): 
        n, t, d = y.shape
        return self.J(out.reshape(n*t,-1), y.reshape(n*t,))

    def predict(self, x, state=None):
        x = x.float()
        x, state = self.lstm(x, state)
        x = self.fc(x)
        return x.softmax(-1), state

model = Net(input_dim, hidden_dim, output_dim, nb_lstm_layers)
nb_param = sum(p.numel() for p in model.parameters())
print(model)
print('# param:    %d' % nb_param)

"""
Optimizer
Scheduler decreases lr every epoch by a factor of 0.1
"""

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=5e-4)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

"""
## Train
"""

trainloader = data.DataLoader(x_train, batch_size=batch_size, shuffle=True, num_workers=0)
devloader = data.DataLoader(x_dev, batch_size=1, shuffle=False, num_workers=0)

loss_train = np.zeros(nb_epochs)
loss_dev = np.zeros(nb_epochs)

model.train()
for i in range(nb_epochs):
    if i > 0:
        scheduler.step()

    # ---- W
    tic = time.time()
    model.train()
    for x in trainloader:

        y = x[:,1:]      # value to predict
        x = x[:,:-1]     # previous values
        
        optimizer.zero_grad()
        out = model(x)
        loss = model.loss(out, y.long())
        loss.backward()
        optimizer.step()

        loss_train[i] += loss.item() /  (len(x_dev_aux) * x.shape[0] * x.shape[1])
    toc = time.time()

    # ---- print
    print("it %d/%d, Jtr = %f, time: %.2fs" % (i, nb_epochs, loss_train[i], toc - tic))

    # ---- dev
    model.eval()
    for x in devloader:

        y = x[:,1:]      # value to predict
        x = x[:,:-1]     # previous values

        out = model(x)
        loss = model.loss(out, y.long())

        loss_dev[i] += loss.item() / (len(x_dev_aux) * x.shape[0] * x.shape[1])

        #print('    Jdev = %f, err = %f\n' % (loss_dev[i], err_dev[i]))
    print('    Jdev = %f\n' % loss_dev[i])

"""
## View loss vs its
"""
"""
plt.figure()
plt.plot(loss_train, 'r')
plt.plot(loss_dev, 'bo--')
plt.ylabel('J')
plt.xlabel('it')
plt.grid(True)
plt.show()
"""
"""
## Save model
"""

torch.save( model, 'model.dat')

"""
Test 
Load model
"""

model = torch.load('model.dat')

"""
Clase que proporciona los métodos necesarios para codificar un mensaje 
utilizando el método Huffman.
"""

class HuffTree(object):
    def __init__(self, w, symbol=None, zero=None, one=None):
        self.w = w
        self.symbol = symbol #symbol only ever populated on leafs
        self.zero = zero
        self.one = one
    
def _combine(tree1, tree2):
    return HuffTree(w = tree1.w+tree2.w, zero=tree1, one=tree2)

def make_huffman_tree(w_):
    return _build([HuffTree(w, i) for i, w in w_] )
    #return _build([HuffTree(w, i) for i, w in enumerate(w_)] )

def _build(nodes):
    if len(nodes) == 1:
        #bottom out if only one node left
        return nodes[0]
    else:
        #otherwise sort the list, take the bottom two elements and combine
        nodes.sort(key = lambda x: x.w, reverse=True)
        n1 = nodes.pop()
        n2 = nodes.pop()
        nodes.append(_combine(n2, n1))
        return _build(nodes)


def flatten_to_dict(tree, codeword=[], code_dict={}):
    if not tree.symbol is None:
        if len(codeword) == 0:
            codeword.append(0)
        code_dict[tree.symbol] = codeword
    else:
        flatten_to_dict(tree.zero, codeword + [0,], code_dict)
        flatten_to_dict(tree.one, codeword + [1,], code_dict)
    return code_dict

def huffman_encode(m_, d):
    return sum([d[m] for m in m_],[])

def huffman_decode(code_message, hufftree):
    decoded=[]
    t = hufftree
    for i in code_message:
        if i == 0:
            t = t.zero
        elif i == 1:
            t = t.one
        else:
            raise Exception("Code_message not binary")

        if not t.symbol is None:
            decoded.append(t.symbol)
            t = hufftree

    return decoded

from math import log2
"""
Clase que permite codificar y decodificar el mensaje, además de comprobar que 
el mensaje original con el decodificado coinciden.
"""

class CodDecod(object):
  def __init__(self, x_true, x_t0, symbols, model):
        self.x_true = x_true
        self.x_t0 = x_t0
        self.symbols = symbols
        self.model = model

  def Code(self):
    s = None
    message_encoded = []
    x_t = self.x_t0
    for t in range(len(self.x_true[0])):
        #print(x_t.shape)
        p, s = self.model.predict(x_t, s)
        
        x_t = torch.FloatTensor([[[self.x_true[0,t]]]])
        # ENCODING PROCESS
        w_ = p.reshape((len(p[0,0]), -1))
        w_ = w_.tolist()
        dictionary = [ (self.symbols[j], w_[j]) for j in range(len(w_)) ]
        h = make_huffman_tree(dictionary)
        
        d = flatten_to_dict(h)
        w = [int(x_true[0, t])]
        u = huffman_encode(w, d)
      
        message_encoded.append(u)
        
        if t in range(0,len(self.x_true[0]),100):
          print('----------------------------------')
          print('t: %d, x_correct: %f, highest prob: %f:' % (t, self.x_true[0, t], p.argmax(2)))
          #print('diccionario: ', d)
          #print('Valor real w: ',w)
          print('palabra codificado: ',u)
          #print("x_t: ", x_t)
          print('----------------------------------')
        
    message_encoded = [item for sublist in message_encoded for item in sublist]
    return message_encoded

  def Decode(self, message_encoded):
    x_t = self.x_t0
    s = None
    message_decoded = []
    word_encoded = []
    i = 0
    for t in range(len(self.x_true[0])):
      p, s = self.model.predict(x_t, s)
      #print('t: %d, x_correct: %f, highest prob: %f: ' % (t, self.x_true[0, t], p.argmax(2)))
      w_ = p.reshape((len(p[0,0]), -1))
      w_ = w_.tolist()
      dictionary = [ (self.symbols[j], w_[j]) for j in range(len(w_)) ]
      h = make_huffman_tree(dictionary) 
      d = flatten_to_dict(h)
      
      #print("Dictionary: ", d)
      # Búsqueda de la siguiente palabra codificada
      word_encoded = []
      word_encoded.extend(message_encoded[i:i+1])
      while ( word_encoded not in d.values() ) and ( i < len(message_encoded)-1 ):
        i = i+1
        word_encoded.extend(message_encoded[i:i+1])
      i = i+1
      #print("Encoded word: ", word_encoded)
      word_decoded = huffman_decode(word_encoded, h)
      message_decoded.append(word_decoded)
      x_t = torch.FloatTensor([[word_decoded]])
    return message_decoded

  def Check(self, message_decoded):
    # ANÁLISIS DE RESULTADOS
    x_true = self.x_true[0].detach().numpy()
    message_decoded = np.array(message_decoded)
    error = abs(message_decoded-x_true)
    """
    plt.plot(error)
    plt.title("Absolute error at each sample")
    plt.show()
    

    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    ax.plot(message_decoded, color ='tab:red')
    ax.plot(x_true, color ='tab:blue')

    ax.set_title('Real data vs Decoded data')
      
    # display the plot
    plt.show()
    """
  def DataCompressionRate(self, message_encoded, output_dim):
    original_bits_sample = math.log2(output_dim)
    encoded_bits_sample = len(message_encoded)/len(self.x_true[0])
    compression_rate = original_bits_sample/encoded_bits_sample
    print("Original size: ", original_bits_sample, "bits/sample")
    print("Encoded size: ", encoded_bits_sample, "bits/sample")
    print("Compression rate: ", compression_rate)

x_true = torch.FloatTensor( x_test[10] )
x_t0 = torch.FloatTensor([[[1]]])
symbols = [i for i in range (output_dim)]

CoderDecoder = CodDecod(x_true, x_t0, symbols, model)
message_encoded = CoderDecoder.Code()
message_decoded = CoderDecoder.Decode(message_encoded)
CoderDecoder.Check(message_decoded)

CoderDecoder.DataCompressionRate(message_encoded, output_dim)